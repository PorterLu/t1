From 0ad6da899c20df0ce6120bc4ebd32edf01bdcd36 Mon Sep 17 00:00:00 2001
From: SharzyL <me@sharzy.in>
Date: Sun, 25 Feb 2024 01:28:18 +0800
Subject: [PATCH] enforce lanewise order for unordered reduce

---
 riscv/insns/vfredusum_vs.h |  2 +-
 riscv/v_ext_macros.h       | 57 ++++++++++++++++++++++++++++++++++++++
 riscv/vector_unit.h        |  4 +++
 3 files changed, 62 insertions(+), 1 deletion(-)

diff --git a/riscv/insns/vfredusum_vs.h b/riscv/insns/vfredusum_vs.h
index bad7308e..3c1fd776 100644
--- a/riscv/insns/vfredusum_vs.h
+++ b/riscv/insns/vfredusum_vs.h
@@ -1,6 +1,6 @@
 // vfredsum: vd[0] =  sum( vs2[*] , vs1[0] )
 bool is_propagate = true;
-VI_VFP_VV_LOOP_REDUCTION
+VI_VFP_VV_LOOP_LANE_ORDERED_REDUCTION
 ({
   vd_0 = f16_add(vd_0, vs2);
 },
diff --git a/riscv/v_ext_macros.h b/riscv/v_ext_macros.h
index b198d54b..e5ea1045 100644
--- a/riscv/v_ext_macros.h
+++ b/riscv/v_ext_macros.h
@@ -1656,6 +1656,63 @@ reg_t index[P.VU.vlmax]; \
       break; \
   }; \
 
+#define VI_VFP_VV_LOOP_REDUCTION_LANE_ORDERED_BASE(width, BODY) \
+  float##width##_t vd_0 = P.VU.elt<float##width##_t>(rd_num, 0); \
+  float##width##_t vs1_0 = P.VU.elt<float##width##_t>(rs1_num, 0); \
+  vd_0 = vs1_0; \
+  bool is_active = false; \
+  reg_t vstart = P.VU.vstart->read(); \
+  reg_t step_size = P.VU.lane_num * P.VU.lane_granularity / width; \
+  for (reg_t l = 0; l < P.VU.lane_num; l++) { \
+    if (l >= vl) continue; \
+    reg_t i = l; \
+    float##width##_t vd_0_backup = vd_0; \
+    bool vd_0_initialized = false; \
+    bool mask = insn.v_vm() == 1 || (P.VU.elt<uint64_t>(0, (i / 64)) >> (i % 64)) & 0x1; \
+    if (mask) { \
+      vd_0 = P.VU.elt<float##width##_t>(rs2_num, i); \
+      vd_0_initialized = true; \
+    } \
+    for (i = l + step_size; i < vl; i += step_size) { \
+      bool mask = insn.v_vm() == 1 || (P.VU.elt<uint64_t>(0, (i / 64)) >> (i % 64)) & 0x1; \
+      if (mask) { \
+        if (vd_0_initialized) { \
+          float##width##_t vs2 = P.VU.elt<float##width##_t>(rs2_num, i); \
+          is_active = true; BODY; set_fp_exceptions; \
+        } else { \
+          vd_0 = P.VU.elt<float##width##_t>(rs2_num, i); \
+          vd_0_initialized = true; \
+        } \
+      } \
+    } \
+    float##width##_t vs2 = vd_0; \
+    vd_0 = vd_0_backup; \
+    if (vd_0_initialized) { \
+      is_active = true; BODY; set_fp_exceptions; \
+    } \
+  /* back brace included */ VI_VFP_LOOP_REDUCTION_END(e32);
+
+#define VI_VFP_VV_LOOP_LANE_ORDERED_REDUCTION(BODY16, BODY32, BODY64) \
+  VI_CHECK_REDUCTION(false) \
+  VI_VFP_COMMON \
+  switch (P.VU.vsew) { \
+    case e16: { \
+      VI_VFP_VV_LOOP_REDUCTION_LANE_ORDERED_BASE(16, BODY16) \
+      break; \
+    } \
+    case e32: { \
+      VI_VFP_VV_LOOP_REDUCTION_LANE_ORDERED_BASE(32, BODY32) \
+      break; \
+    } \
+    case e64: { \
+      VI_VFP_VV_LOOP_REDUCTION_LANE_ORDERED_BASE(64, BODY64) \
+      break; \
+    } \
+    default: \
+      require(0); \
+      break; \
+  }; \
+
 #define VI_VFP_VV_LOOP_WIDE_REDUCTION(BODY16, BODY32) \
   VI_CHECK_REDUCTION(true) \
   VI_VFP_COMMON \
diff --git a/riscv/vector_unit.h b/riscv/vector_unit.h
index a057c62f..152643dc 100644
--- a/riscv/vector_unit.h
+++ b/riscv/vector_unit.h
@@ -102,6 +102,9 @@ public:
   bool vill;
   bool vstart_alu;
 
+  reg_t lane_num = 8;
+  reg_t lane_granularity = 32;
+
   // vector element for various SEW
   template<class T> T& elt(reg_t vReg, reg_t n, bool is_write = false);
   // vector element group access, where EG is a std::array<T, N>.
@@ -149,4 +152,5 @@ public:
     return (VRM)(vxrm->read());
   }
 };
+
 #endif
-- 
2.43.0

diff --git a/riscv/mmu.cc b/riscv/mmu.cc
index d10e23ae..f4c7f68b 100644
--- a/riscv/mmu.cc
+++ b/riscv/mmu.cc
@@ -62,7 +62,11 @@ reg_t mmu_t::translate(mem_access_info_t access_info, reg_t len)
   bool virt = access_info.effective_virt;
   reg_t mode = (reg_t) access_info.effective_priv;
 
+  printf("addr: %x, virt:%x, mode:%x\n", addr, virt, mode);
+
   reg_t paddr = walk(access_info) | (addr & (PGSIZE-1));
+
+  printf("paddr: %x\n", paddr);
   if (!pmp_ok(paddr, len, access_info.flags.ss_access ? STORE : type, mode, access_info.flags.hlvx))
     throw_access_exception(virt, addr, type);
   return paddr;
@@ -70,13 +74,16 @@ reg_t mmu_t::translate(mem_access_info_t access_info, reg_t len)
 
 tlb_entry_t mmu_t::fetch_slow_path(reg_t vaddr)
 {
+  printf("PTW\n");
   auto access_info = generate_access_info(vaddr, FETCH, {});
   check_triggers(triggers::OPERATION_EXECUTE, vaddr, access_info.effective_virt);
 
   tlb_entry_t result;
   reg_t vpn = vaddr >> PGSHIFT;
   if (unlikely(tlb_insn_tag[vpn % TLB_ENTRIES] != (vpn | TLB_CHECK_TRIGGERS))) {
+    printf("vpn: %x\n", vpn);
     reg_t paddr = translate(access_info, sizeof(fetch_temp));
+    printf("translate over, paddr: %x\n", paddr);
     if (auto host_addr = sim->addr_to_mem(paddr)) {
       result = refill_tlb(vaddr, paddr, host_addr, FETCH);
     } else {
@@ -593,6 +600,8 @@ reg_t mmu_t::walk(mem_access_info_t access_info)
     }
   }
 
+
+  printf("%x\n", type);
   switch (type) {
     case FETCH: throw trap_instruction_page_fault(virt, addr, 0, 0);
     case LOAD: throw trap_load_page_fault(virt, addr, 0, 0);
diff --git a/riscv/mmu.h b/riscv/mmu.h
index 3e4ae9a7..c6383eaa 100644
--- a/riscv/mmu.h
+++ b/riscv/mmu.h
@@ -308,7 +308,10 @@ public:
     if (matched_trigger)
       throw *matched_trigger;
 
+    printf("TLB Translate Addr: %x\n", addr);
     auto tlb_entry = translate_insn_addr(addr);
+    printf("TLB Translate Over\n");
+
     insn_bits_t insn = from_le(*(uint16_t*)(tlb_entry.host_offset + addr));
     int length = insn_length(insn);
 
diff --git a/riscv/trap.h b/riscv/trap.h
index 5eb62cfd..7004f87a 100644
--- a/riscv/trap.h
+++ b/riscv/trap.h
@@ -91,6 +91,7 @@ class mem_trap_t : public trap_t
 #define DECLARE_MEM_TRAP(n, x) class trap_##x : public mem_trap_t { \
  public: \
   trap_##x(bool gva, reg_t tval, reg_t tval2, reg_t tinst) : mem_trap_t(n, gva, tval, tval2, tinst) {} \
+  printf("%s\n", "trap_"#x); \
   std::string name() { return "trap_"#x; } \
 };

